"""
Custom Prometheus Exporter –¥–ª—è –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞
"""
import os
import time
import asyncio
import httpx
from prometheus_client import start_http_server, Gauge, Counter, Info
from prometheus_client.core import GaugeMetricFamily, REGISTRY
from typing import Dict, Any

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
BACKEND_URL = os.getenv("BACKEND_URL", "http://backend:8000")
METRICS_ENDPOINT = f"{BACKEND_URL}/api/v1/metrics/business"
SCRAPE_INTERVAL = int(os.getenv("SCRAPE_INTERVAL", "30"))  # —Å–µ–∫—É–Ω–¥—ã
EXPORTER_PORT = int(os.getenv("EXPORTER_PORT", "9100"))

# –ú–µ—Ç—Ä–∏–∫–∏ Prometheus
# –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏
users_total = Gauge('medhistory_users_total', '–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π')
users_active_30d = Gauge('medhistory_users_active_30d', '–ê–∫—Ç–∏–≤–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –∑–∞ 30 –¥–Ω–µ–π')
users_new_30d = Gauge('medhistory_users_new_30d', '–ù–æ–≤—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –∑–∞ 30 –¥–Ω–µ–π')

# –î–æ–∫—É–º–µ–Ω—Ç—ã
documents_total = Gauge('medhistory_documents_total', '–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤')
documents_new_30d = Gauge('medhistory_documents_new_30d', '–ù–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∑–∞ 30 –¥–Ω–µ–π')
documents_by_type = Gauge('medhistory_documents_by_type', '–î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Ç–∏–ø–∞–º', ['document_type'])

# –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ AI
interpretations_total = Gauge('medhistory_interpretations_total', '–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–π')
interpretations_success = Gauge('medhistory_interpretations_success_total', '–£—Å–ø–µ—à–Ω—ã–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏')
interpretations_failed = Gauge('medhistory_interpretations_failed_total', '–ù–µ—É–¥–∞—á–Ω—ã–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏')
interpretations_new_30d = Gauge('medhistory_interpretations_new_30d', '–ù–æ–≤—ã–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∑–∞ 30 –¥–Ω–µ–π')

# –û—Ç—á–µ—Ç—ã
reports_total = Gauge('medhistory_reports_total', '–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç—á–µ—Ç–æ–≤')
reports_new_30d = Gauge('medhistory_reports_new_30d', '–ù–æ–≤—ã–µ –æ—Ç—á–µ—Ç—ã –∑–∞ 30 –¥–Ω–µ–π')

# –•—Ä–∞–Ω–∏–ª–∏—â–µ
storage_bytes = Gauge('medhistory_storage_bytes', '–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –≤ –±–∞–π—Ç–∞—Ö')
storage_objects = Gauge('medhistory_storage_objects', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ')

# –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ä–∞–±–æ—Ç—ã —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä–∞
scrape_success = Counter('medhistory_exporter_scrape_success_total', '–£—Å–ø–µ—à–Ω—ã–µ —Å–±–æ—Ä—ã –º–µ—Ç—Ä–∏–∫')
scrape_errors = Counter('medhistory_exporter_scrape_errors_total', '–û—à–∏–±–∫–∏ –ø—Ä–∏ —Å–±–æ—Ä–µ –º–µ—Ç—Ä–∏–∫')
scrape_duration = Gauge('medhistory_exporter_scrape_duration_seconds', '–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫')

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä–µ
exporter_info = Info('medhistory_exporter', '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä–µ')
exporter_info.info({
    'version': '1.0.0',
    'backend_url': BACKEND_URL,
    'scrape_interval': str(SCRAPE_INTERVAL)
})


async def fetch_metrics() -> Dict[str, Any]:
    """
    –ü–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ Backend API
    """
    async with httpx.AsyncClient(timeout=30.0) as client:
        try:
            response = await client.get(METRICS_ENDPOINT)
            response.raise_for_status()
            return response.json()
        except httpx.HTTPError as e:
            print(f"‚ùå HTTP Error fetching metrics: {e}")
            raise
        except Exception as e:
            print(f"‚ùå Error fetching metrics: {e}")
            raise


def update_metrics(data: Dict[str, Any]):
    """
    –û–±–Ω–æ–≤–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ Prometheus –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    try:
        # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏
        users_data = data.get('users', {})
        users_total.set(users_data.get('total', 0))
        users_active_30d.set(users_data.get('active_30d', 0))
        users_new_30d.set(users_data.get('new_30d', 0))
        
        # –î–æ–∫—É–º–µ–Ω—Ç—ã
        documents_data = data.get('documents', {})
        documents_total.set(documents_data.get('total', 0))
        documents_new_30d.set(documents_data.get('new_30d', 0))
        
        # –î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Ç–∏–ø–∞–º
        by_type = documents_data.get('by_type', {})
        for doc_type, count in by_type.items():
            documents_by_type.labels(document_type=doc_type).set(count)
        
        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
        interpretations_data = data.get('interpretations', {})
        interpretations_total.set(interpretations_data.get('total', 0))
        interpretations_success.set(interpretations_data.get('success', 0))
        interpretations_failed.set(interpretations_data.get('failed', 0))
        interpretations_new_30d.set(interpretations_data.get('new_30d', 0))
        
        # –û—Ç—á–µ—Ç—ã
        reports_data = data.get('reports', {})
        reports_total.set(reports_data.get('total', 0))
        reports_new_30d.set(reports_data.get('new_30d', 0))
        
        # –•—Ä–∞–Ω–∏–ª–∏—â–µ
        storage_data = data.get('storage', {})
        storage_bytes.set(storage_data.get('bytes', 0))
        storage_objects.set(storage_data.get('objects', 0))
        
        print(f"‚úÖ Metrics updated successfully at {time.strftime('%Y-%m-%d %H:%M:%S')}")
        
    except Exception as e:
        print(f"‚ùå Error updating metrics: {e}")
        raise


async def collect_metrics_loop():
    """
    –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫
    """
    print(f"üöÄ Starting metrics collection loop (interval: {SCRAPE_INTERVAL}s)")
    
    while True:
        start_time = time.time()
        
        try:
            # –ü–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ API
            data = await fetch_metrics()
            
            # –û–±–Ω–æ–≤–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ Prometheus
            update_metrics(data)
            
            # –ó–∞–ø–∏—Å–∞—Ç—å —É—Å–ø–µ—à–Ω—ã–π —Å–±–æ—Ä
            scrape_success.inc()
            
        except Exception as e:
            print(f"‚ùå Error in metrics collection: {e}")
            scrape_errors.inc()
        
        finally:
            # –ó–∞–ø–∏—Å–∞—Ç—å –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–±–æ—Ä–∞
            duration = time.time() - start_time
            scrape_duration.set(duration)
        
        # –ü–æ–¥–æ–∂–¥–∞—Ç—å –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–±–æ—Ä–∞
        await asyncio.sleep(SCRAPE_INTERVAL)


def main():
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä–∞
    """
    print("=" * 60)
    print("üè• MedHistory Custom Metrics Exporter")
    print("=" * 60)
    print(f"üìä Exporter Port: {EXPORTER_PORT}")
    print(f"üîó Backend URL: {BACKEND_URL}")
    print(f"‚è±Ô∏è  Scrape Interval: {SCRAPE_INTERVAL}s")
    print("=" * 60)
    
    # –ó–∞–ø—É—Å—Ç–∏—Ç—å HTTP —Å–µ—Ä–≤–µ—Ä –¥–ª—è Prometheus
    start_http_server(EXPORTER_PORT)
    print(f"‚úÖ Prometheus metrics server started on port {EXPORTER_PORT}")
    print(f"üìà Metrics available at http://localhost:{EXPORTER_PORT}/metrics")
    
    # –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ü–∏–∫–ª —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫
    try:
        asyncio.run(collect_metrics_loop())
    except KeyboardInterrupt:
        print("\nüõë Exporter stopped by user")
    except Exception as e:
        print(f"‚ùå Fatal error: {e}")
        raise


if __name__ == "__main__":
    main()

