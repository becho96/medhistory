import base64
import httpx
import json
from typing import Optional
from datetime import datetime
from io import BytesIO
import PyPDF2

from app.core.config import settings
from app.schemas.document import DocumentMetadata

class AIService:
    def __init__(self):
        self.api_key = settings.OPENROUTER_API_KEY
        self.model = settings.OPENROUTER_MODEL
        self.base_url = settings.OPENROUTER_BASE_URL
    
    async def analyze_document(self, file_bytes: bytes, file_type: str, filename: str) -> DocumentMetadata:
        """Analyze document and extract metadata using AI"""
        
        try:
            # Build prompt
            prompt = self._build_extraction_prompt()
            
            # Handle different file types
            if file_type == 'pdf':
                # Extract text from PDF
                print("üìÑ –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ PDF...")
                text_content = self._extract_text_from_pdf(file_bytes)
                
                if not text_content or len(text_content.strip()) < 50:
                    raise ValueError("PDF —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ —ç—Ç–æ –æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –¢—Ä–µ–±—É–µ—Ç—Å—è OCR.")
                
                print(f"  ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(text_content)} —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞")
                print(f"  üìù –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤: {text_content[:200]}...")
                
                # Prepare text-only message
                messages = [
                    {
                        "role": "user",
                        "content": f"{prompt}\n\n–¢–µ–∫—Å—Ç –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞:\n\n{text_content}"
                    }
                ]
                
            elif file_type in ['jpg', 'jpeg', 'png']:
                # Use vision API for images
                print(f"üñºÔ∏è –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ ({file_type})...")
                
                # Encode file to base64
                file_base64 = base64.b64encode(file_bytes).decode('utf-8')
                
                # Determine content type
                if file_type in ['jpg', 'jpeg']:
                    mime_type = 'image/jpeg'
                else:
                    mime_type = 'image/png'
                
                # Prepare vision message
                messages = [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:{mime_type};base64,{file_base64}"
                                }
                            }
                        ]
                    }
                ]
            else:
                raise ValueError(f"Unsupported file type: {file_type}")
            
            # Call OpenRouter API
            response_data = await self._call_openrouter(messages)
            
            # Parse response
            metadata = self._parse_ai_response(response_data)
            
            return metadata
            
        except Exception as e:
            print(f"‚ùå AI analysis failed: {str(e)}")
            # Return default metadata on error
            return DocumentMetadata(
                document_type="–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
                confidence=0.0,
                summary=f"–ù–µ —É–¥–∞–ª–æ—Å—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç: {str(e)}"
            )
    
    def _extract_text_from_pdf(self, file_bytes: bytes) -> str:
        """Extract text content from PDF file"""
        try:
            pdf_file = BytesIO(file_bytes)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            
            text_parts = []
            for page_num in range(len(pdf_reader.pages)):
                page = pdf_reader.pages[page_num]
                text = page.extract_text()
                if text:
                    text_parts.append(text)
            
            full_text = "\n\n".join(text_parts)
            return full_text.strip()
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF: {e}")
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ PDF: {str(e)}")
    
    def _build_extraction_prompt(self) -> str:
        return """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ—Ç –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ –∏–∑–≤–ª–µ–∫–∏ —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON.

# –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–ê–Ø –°–¢–†–£–ö–¢–£–†–ê JSON:

{
  "document_type": "–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û - –æ–¥–∏–Ω –∏–∑ 5 —Ç–∏–ø–æ–≤",
  "document_subtype": "—É—Å–ª–æ–≤–Ω–æ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ",
  "research_area": "—Ç–æ–ª—å–∫–æ –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π",
  "specialties": ["–º–∞—Å—Å–∏–≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–µ–π"],
  "document_date": "YYYY-MM-DD",
  "patient_name": "–§–∞–º–∏–ª–∏—è –ò.–û.",
  "medical_facility": "–Ω–∞–∑–≤–∞–Ω–∏–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏—è",
  "document_language": "ru|en",
  "confidence": 0.95,
  "summary": "–∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ"
}

# –¢–ò–ü–´ –î–û–ö–£–ú–ï–ù–¢–û–í (–≤—ã–±–µ—Ä–∏ –û–î–ò–ù –∏–∑ 5):

1. "–ü—Ä–∏–µ–º –≤—Ä–∞—á–∞" - –∞–º–±—É–ª–∞—Ç–æ—Ä–Ω—ã–π –ø—Ä–∏–µ–º, –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è, –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –≤—Ä–∞—á–∞
2. "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞" - –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã–µ –∞–Ω–∞–ª–∏–∑—ã –ª—é–±–æ–≥–æ —Ç–∏–ø–∞
3. "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ" - –£–ó–ò, –ú–†–¢, –ö–¢, —Ä–µ–Ω—Ç–≥–µ–Ω, –º–∞–º–º–æ–≥—Ä–∞—Ñ–∏—è
4. "–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞" - –≠–ö–ì, –≠–≠–ì, —Ö–æ–ª—Ç–µ—Ä, —Å–ø–∏—Ä–æ–º–µ—Ç—Ä–∏—è, –§–ì–î–°, –∫–æ–ª–æ–Ω–æ—Å–∫–æ–ø–∏—è
5. "–î—Ä—É–≥–æ–µ" - –µ—Å–ª–∏ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –Ω–∏ –ø–æ–¥ –æ–¥–Ω—É –∫–∞—Ç–µ–≥–æ—Ä–∏—é

# –ü–†–ê–í–ò–õ–ê –ó–ê–ü–û–õ–ù–ï–ù–ò–Ø –ü–û –¢–ò–ü–ê–ú:

## "–ü—Ä–∏–µ–º –≤—Ä–∞—á–∞":
- specialties: ["–ö–∞—Ä–¥–∏–æ–ª–æ–≥–∏—è", "–¢–µ—Ä–∞–ø–∏—è"] - –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –º–∞—Å—Å–∏–≤ (–º–∏–Ω–∏–º—É–º 1)
- document_subtype: null
- research_area: null

–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–µ–π: –¢–µ—Ä–∞–ø–∏—è, –ö–∞—Ä–¥–∏–æ–ª–æ–≥–∏—è, –ù–µ–≤—Ä–æ–ª–æ–≥–∏—è, –ì–∞—Å—Ç—Ä–æ—ç–Ω—Ç–µ—Ä–æ–ª–æ–≥–∏—è, –≠–Ω–¥–æ–∫—Ä–∏–Ω–æ–ª–æ–≥–∏—è, –£—Ä–æ–ª–æ–≥–∏—è, –û—Ç–æ—Ä–∏–Ω–æ–ª–∞—Ä–∏–Ω–≥–æ–ª–æ–≥–∏—è (–õ–û–†), –û—Ñ—Ç–∞–ª—å–º–æ–ª–æ–≥–∏—è, –¢—Ä–∞–≤–º–∞—Ç–æ–ª–æ–≥–∏—è –∏ –æ—Ä—Ç–æ–ø–µ–¥–∏—è, –î–µ—Ä–º–∞—Ç–æ–ª–æ–≥–∏—è, –ò–Ω—Ñ–µ–∫—Ü–∏–æ–Ω–Ω—ã–µ –±–æ–ª–µ–∑–Ω–∏, –†–µ–≤–º–∞—Ç–æ–ª–æ–≥–∏—è, –ü—É–ª—å–º–æ–Ω–æ–ª–æ–≥–∏—è, –ù–µ—Ñ—Ä–æ–ª–æ–≥–∏—è, –û–Ω–∫–æ–ª–æ–≥–∏—è, –•–∏—Ä—É—Ä–≥–∏—è, –ê–∫—É—à–µ—Ä—Å—Ç–≤–æ –∏ –≥–∏–Ω–µ–∫–æ–ª–æ–≥–∏—è, –ü–µ–¥–∏–∞—Ç—Ä–∏—è, –ü—Å–∏—Ö–∏–∞—Ç—Ä–∏—è, –î—Ä—É–≥–∞—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å

## "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞":
- document_subtype: "–û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –∫—Ä–æ–≤–∏" - –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û
- specialties: null
- research_area: null

–ü–æ–¥—Ç–∏–ø—ã: –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –∫—Ä–æ–≤–∏, –ë–∏–æ—Ö–∏–º–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫—Ä–æ–≤–∏, –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –º–æ—á–∏, –ê–Ω–∞–ª–∏–∑ –∫–∞–ª–∞, –ë–∞–∫—Ç–µ—Ä–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑, –°–µ—Ä–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑, –ì–∏—Å—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, –¶–∏—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, –ò–º–º—É–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑, –ì–æ—Ä–º–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑, –ì–µ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑, –î—Ä—É–≥–æ–π –∞–Ω–∞–ª–∏–∑

## "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ":
- document_subtype: "–£–ó–ò" - –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û
- research_area: "–ë—Ä—é—à–Ω–∞—è –ø–æ–ª–æ—Å—Ç—å" - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è
- specialties: null

–ü–æ–¥—Ç–∏–ø—ã: –£–ó–ò, –ú–†–¢, –ö–¢, –†–µ–Ω—Ç–≥–µ–Ω, –ú–∞–º–º–æ–≥—Ä–∞—Ñ–∏—è, –§–ª—é–æ—Ä–æ–≥—Ä–∞—Ñ–∏—è, –ê–Ω–≥–∏–æ–≥—Ä–∞—Ñ–∏—è, –î—Ä—É–≥–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ

–û–±–ª–∞—Å—Ç–∏ –¥–ª—è –£–ó–ò: –ë—Ä—é—à–Ω–∞—è –ø–æ–ª–æ—Å—Ç—å, –©–∏—Ç–æ–≤–∏–¥–Ω–∞—è –∂–µ–ª–µ–∑–∞, –ú–æ–ª–æ—á–Ω—ã–µ –∂–µ–ª–µ–∑—ã, –û—Ä–≥–∞–Ω—ã –º–∞–ª–æ–≥–æ —Ç–∞–∑–∞, –°–µ—Ä–¥—Ü–µ (–≠—Ö–æ–ö–ì), –°–æ—Å—É–¥—ã, –°—É—Å—Ç–∞–≤—ã, –ú—è–≥–∫–∏–µ —Ç–∫–∞–Ω–∏, –î—Ä—É–≥–∞—è –æ–±–ª–∞—Å—Ç—å

–û–±–ª–∞—Å—Ç–∏ –¥–ª—è –ú–†–¢/–ö–¢: –ì–æ–ª–æ–≤–Ω–æ–π –º–æ–∑–≥, –ü–æ–∑–≤–æ–Ω–æ—á–Ω–∏–∫, –ë—Ä—é—à–Ω–∞—è –ø–æ–ª–æ—Å—Ç—å, –ì—Ä—É–¥–Ω–∞—è –∫–ª–µ—Ç–∫–∞, –°—É—Å—Ç–∞–≤—ã, –ú—è–≥–∫–∏–µ —Ç–∫–∞–Ω–∏, –°–æ—Å—É–¥—ã, –î—Ä—É–≥–∞—è –æ–±–ª–∞—Å—Ç—å

–û–±–ª–∞—Å—Ç–∏ –¥–ª—è –†–µ–Ω—Ç–≥–µ–Ω: –ì—Ä—É–¥–Ω–∞—è –∫–ª–µ—Ç–∫–∞, –ü–æ–∑–≤–æ–Ω–æ—á–Ω–∏–∫, –°—É—Å—Ç–∞–≤—ã, –ö–æ—Å—Ç–∏ –∫–æ–Ω–µ—á–Ω–æ—Å—Ç–µ–π, –ß–µ—Ä–µ–ø–∞, –î—Ä—É–≥–∞—è –æ–±–ª–∞—Å—Ç—å

## "–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞":
- document_subtype: "–≠–ö–ì (—ç–ª–µ–∫—Ç—Ä–æ–∫–∞—Ä–¥–∏–æ–≥—Ä–∞—Ñ–∏—è)" - –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û
- specialties: null
- research_area: null

–ü–æ–¥—Ç–∏–ø—ã: –≠–ö–ì (—ç–ª–µ–∫—Ç—Ä–æ–∫–∞—Ä–¥–∏–æ–≥—Ä–∞—Ñ–∏—è), –≠–≠–ì (—ç–ª–µ–∫—Ç—Ä–æ—ç–Ω—Ü–µ—Ñ–∞–ª–æ–≥—Ä–∞—Ñ–∏—è), –•–æ–ª—Ç–µ—Ä-–º–æ–Ω–∏—Ç–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ, –°—É—Ç–æ—á–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ê–î, –°–ø–∏—Ä–æ–º–µ—Ç—Ä–∏—è, –í–µ–ª–æ—ç—Ä–≥–æ–º–µ—Ç—Ä–∏—è, –§–ì–î–° (—Ñ–∏–±—Ä–æ–≥–∞—Å—Ç—Ä–æ–¥—É–æ–¥–µ–Ω–æ—Å–∫–æ–ø–∏—è), –ö–æ–ª–æ–Ω–æ—Å–∫–æ–ø–∏—è, –ë—Ä–æ–Ω—Ö–æ—Å–∫–æ–ø–∏—è, –î—Ä—É–≥–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞

## "–î—Ä—É–≥–æ–µ":
- –í—Å–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã
- confidence –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –Ω–∏–∑–∫–æ–π (<0.5)

# –û–ë–©–ò–ï –ü–†–ê–í–ò–õ–ê:

1. document_type - –í–°–ï–ì–î–ê –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û, –∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ—á–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–∑ —Å–ø–∏—Å–∫–∞ –≤—ã—à–µ
2. –î–∞—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ - —ç—Ç–æ –¥–∞—Ç–∞ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã/–ø—Ä–∏–µ–º–∞, –ù–ï –¥–∞—Ç–∞ –ø–µ—á–∞—Ç–∏
3. –§–æ—Ä–º–∞—Ç –§–ò–û: "–ò–≤–∞–Ω–æ–≤ –ò.–ò." (—Å —Ç–æ—á–∫–∞–º–∏)
4. –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏ (specialties) - –°–£–©–ï–°–¢–í–ò–¢–ï–õ–¨–ù–´–ï: "–ö–∞—Ä–¥–∏–æ–ª–æ–≥–∏—è", –∞ –ù–ï "–ö–∞—Ä–¥–∏–æ–ª–æ–≥"
5. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ - –∏—Å–ø–æ–ª—å–∑—É–π null
6. confidence (0.0-1.0) - —Ç–≤–æ—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
7. document_language - –∫–æ–¥ —è–∑—ã–∫–∞ ("ru", "en")

# –í–ê–ñ–ù–û:
- –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON
- –ë–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
- –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç–∏–ø—É –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    
    async def _call_openrouter(self, messages: list) -> dict:
        """Make API call to OpenRouter"""
        
        payload = {
            "model": self.model,
            "messages": messages,
            "max_tokens": 2000,
            "temperature": 0.1  # Low temperature for consistent extraction
        }
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "http://localhost:8000",
            "X-Title": "MedHistory"
        }
        
        # Debug logging
        print(f"üîç OpenRouter Request:")
        print(f"  URL: {self.base_url}")
        print(f"  Model: {self.model}")
        print(f"  API Key: {self.api_key[:10]}...{self.api_key[-10:]}")
        print(f"  Message content types: {[type(m['content']) for m in messages]}")
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            try:
                response = await client.post(
                    self.base_url,
                    headers=headers,
                    json=payload
                )
                
                # Log response details
                print(f"üì• OpenRouter Response:")
                print(f"  Status: {response.status_code}")
                print(f"  Response: {response.text[:500]}")
                
                response.raise_for_status()
                return response.json()
            except httpx.HTTPStatusError as e:
                print(f"‚ùå HTTP Error: {e.response.status_code}")
                print(f"   Response body: {e.response.text}")
                raise
            except Exception as e:
                print(f"‚ùå Request Error: {str(e)}")
                raise
    
    def _parse_ai_response(self, response_data: dict) -> DocumentMetadata:
        """Parse AI response and create DocumentMetadata"""
        
        try:
            # Extract content from response
            content = response_data["choices"][0]["message"]["content"]
            
            # Parse JSON from content
            # Sometimes the model wraps JSON in markdown code blocks
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            
            data = json.loads(content)
            
            # Create DocumentMetadata object with new structure
            metadata = DocumentMetadata(
                # PostgreSQL fields
                document_type=data.get("document_type", "–î—Ä—É–≥–æ–µ"),
                document_date=data.get("document_date"),
                patient_name=data.get("patient_name"),
                medical_facility=data.get("medical_facility"),
                
                # MongoDB classification fields
                document_subtype=data.get("document_subtype"),
                research_area=data.get("research_area"),
                specialties=data.get("specialties"),
                document_language=data.get("document_language", "ru"),
                confidence=data.get("confidence", 0.5),
                
                # MongoDB extracted_data fields
                summary=data.get("summary")
            )
            
            return metadata
            
        except Exception as e:
            print(f"‚ùå Error parsing AI response: {str(e)}")
            print(f"Response content: {response_data}")
            raise
    
    async def generate_report_content(self, documents: list, filters: dict) -> str:
        """Generate report content using AI"""
        
        # Prepare context from documents
        context = self._prepare_report_context(documents)
        
        prompt = f"""–°–æ–∑–¥–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –æ—Ç—á—ë—Ç –¥–ª—è –ø–∞—Ü–∏–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:

{context}

–û—Ç—á—ë—Ç –¥–æ–ª–∂–µ–Ω –≤–∫–ª—é—á–∞—Ç—å:
1. –û–±—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞—Ü–∏–µ–Ω—Ç–µ
2. –•—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é –≤—Ä–µ–º–µ–Ω–Ω—É—é –ª–∏–Ω–∏—é —Å–æ–±—ã—Ç–∏–π
3. –ö–ª—é—á–µ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–æ–≤ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
4. –ù–∞–∑–Ω–∞—á–µ–Ω–Ω–æ–µ –ª–µ—á–µ–Ω–∏–µ –∏ –µ–≥–æ –¥–∏–Ω–∞–º–∏–∫—É
5. –†–µ–∑—é–º–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∑–¥–æ—Ä–æ–≤—å—è

–§–æ—Ä–º–∞—Ç –æ—Ç—á—ë—Ç–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Å —á–µ—Ç–∫–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏.
–ü–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
–ë—É–¥—å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º –∏ –¥–µ—Ç–∞–ª—å–Ω—ã–º."""
        
        messages = [
            {
                "role": "user",
                "content": prompt
            }
        ]
        
        response_data = await self._call_openrouter(messages)
        content = response_data["choices"][0]["message"]["content"]
        
        return content
    
    def _prepare_report_context(self, documents: list) -> str:
        """Prepare context from documents for report generation"""
        
        context_parts = []
        
        for doc in documents:
            doc_info = f"""
–î–∞—Ç–∞: {doc.document_date or '–Ω–µ —É–∫–∞–∑–∞–Ω–∞'}
–¢–∏–ø: {doc.document_type or '–Ω–µ —É–∫–∞–∑–∞–Ω'}
–ü–∞—Ü–∏–µ–Ω—Ç: {doc.patient_name or '–Ω–µ —É–∫–∞–∑–∞–Ω'}
–£—á—Ä–µ–∂–¥–µ–Ω–∏–µ: {doc.medical_facility or '–Ω–µ —É–∫–∞–∑–∞–Ω–æ'}
–§–∞–π–ª: {doc.original_filename}
"""
            context_parts.append(doc_info)
        
        return "\n---\n".join(context_parts)

    async def extract_lab_results(self, file_bytes: bytes, file_type: str, filename: str) -> dict:
        """Extract laboratory results using a specialized prompt.

        Returns a dict with key "lab_results": list of standardized entries.
        """
        # Build labs prompt
        prompt = self._build_labs_extraction_prompt()

        # Prepare messages similarly to analyze_document
        if file_type == 'pdf':
            text_content = self._extract_text_from_pdf(file_bytes)
            messages = [
                {
                    "role": "user",
                    "content": f"{prompt}\n\n–¢–µ–∫—Å—Ç –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞:\n\n{text_content}"
                }
            ]
        elif file_type in ['jpg', 'jpeg', 'png']:
            file_base64 = base64.b64encode(file_bytes).decode('utf-8')
            mime_type = 'image/jpeg' if file_type in ['jpg', 'jpeg'] else 'image/png'
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": f"data:{mime_type};base64,{file_base64}"}},
                    ],
                }
            ]
        else:
            raise ValueError(f"Unsupported file type: {file_type}")

        response_data = await self._call_openrouter(messages)
        return self._parse_labs_response(response_data)

    def _build_labs_extraction_prompt(self) -> str:
        return """–û–ø—Ä–µ–¥–µ–ª–∏, –µ—Å—Ç—å –ª–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–æ–≤. –ï—Å–ª–∏ –¥–∞, –∏–∑–≤–ª–µ–∫–∏ –∏—Ö –≤ JSON –ø–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Å—Ö–µ–º–µ.

–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç:
{
  "lab_results": [
    {
      "test_name": "–Ω–∞–∑–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: –≥–µ–º–æ–≥–ª–æ–±–∏–Ω, –≥–ª—é–∫–æ–∑–∞)",
      "value": "—á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞",
      "unit": "–µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä: –≥/–ª, –º–º–æ–ª—å/–ª, %)",
      "reference_range": "—Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: 120-160)",
      "flag": "L|N|H|A"  // L –Ω–∏–∂–µ –Ω–æ—Ä–º—ã, N –Ω–æ—Ä–º–∞, H –≤—ã—à–µ –Ω–æ—Ä–º—ã, A –∞–±–Ω–æ—Ä–º–∞–ª—å–Ω–æ/–∫—Ä–∏—Ç–∏—á–Ω–æ
    }
  ]
}

–ü—Ä–∞–≤–∏–ª–∞:
- –ï—Å–ª–∏ –∞–Ω–∞–ª–∏–∑–æ–≤ –Ω–µ—Ç, –≤–µ—Ä–Ω–∏ {"lab_results": []}
- test_name –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –∫–∞–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ, –Ω–æ –±–µ–∑ –ª–∏—à–Ω–µ–≥–æ —à—É–º–∞
- value –æ—Å—Ç–∞–≤–ª—è–π —Å—Ç—Ä–æ–∫–æ–π, –Ω–µ –æ–∫—Ä—É–≥–ª—è–π
- –í–ê–ñ–ù–û: unit –¥–æ–ª–∂–µ–Ω —Ç–æ—á–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –µ–¥–∏–Ω–∏—Ü–∞–º –∏–∑–º–µ—Ä–µ–Ω–∏—è –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
  * –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã –ø—Ä–æ—Ü–µ–Ω—Ç—ã (%), –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∑—ã–≤–∞–π "%" –≤ unit
  * –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, 10*9/–ª, —Ö10^9/–ª, –≥/–ª), —É–∫–∞–∑—ã–≤–∞–π –∏—Ö —Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
  * –†–∞–∑–Ω—ã–µ –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è –æ–¥–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–µ –∏ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ) - —ç—Ç–æ –†–ê–ó–ù–´–ï –±–∏–æ–º–∞—Ä–∫–µ—Ä—ã
  * –ù–µ —Å–º–µ—à–∏–≤–∞–π –µ–¥–∏–Ω–∏—Ü—ã: –µ—Å–ª–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ "–õ–∏–º—Ñ–æ—Ü–∏—Ç—ã 42%" –∏ "–õ–∏–º—Ñ–æ—Ü–∏—Ç—ã 2.5 10*9/–ª", —ç—Ç–æ –¥–≤–µ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∑–∞–ø–∏—Å–∏
- –ï—Å–ª–∏ –µ–¥–∏–Ω–∏—Ü –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–π null
- –ï—Å–ª–∏ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–π null
- –û–ø—Ä–µ–¥–µ–ª–∏ flag –∏—Å—Ö–æ–¥—è –∏–∑ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞, –µ—Å–ª–∏ –æ–Ω —É–∫–∞–∑–∞–Ω
- –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON

–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è:
- "–õ–∏–º—Ñ–æ—Ü–∏—Ç—ã 42%" ‚Üí {"test_name": "–õ–∏–º—Ñ–æ—Ü–∏—Ç—ã", "value": "42", "unit": "%", ...}
- "–õ–∏–º—Ñ–æ—Ü–∏—Ç—ã 2.5 10*9/–ª" ‚Üí {"test_name": "–õ–∏–º—Ñ–æ—Ü–∏—Ç—ã", "value": "2.5", "unit": "10*9/–ª", ...}
- "–ì–µ–º–æ–≥–ª–æ–±–∏–Ω 145 –≥/–ª" ‚Üí {"test_name": "–ì–µ–º–æ–≥–ª–æ–±–∏–Ω", "value": "145", "unit": "–≥/–ª", ...}
- "–ì–µ–º–æ–≥–ª–æ–±–∏–Ω 14.5 –≥/–¥–ª" ‚Üí {"test_name": "–ì–µ–º–æ–≥–ª–æ–±–∏–Ω", "value": "14.5", "unit": "–≥/–¥–ª", ...}
"""

    def _parse_labs_response(self, response_data: dict) -> dict:
        try:
            content = response_data["choices"][0]["message"]["content"]
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            data = json.loads(content)

            # Basic normalization
            lab_results = data.get("lab_results") or []
            normalized = []
            for item in lab_results:
                normalized.append({
                    "test_name": item.get("test_name"),
                    "value": item.get("value"),
                    "unit": item.get("unit"),
                    "reference_range": item.get("reference_range"),
                    "flag": item.get("flag"),
                })
            return {"lab_results": normalized}
        except Exception as e:
            print(f"‚ùå Error parsing labs response: {e}")
            print(f"Response content: {response_data}")
            return {"lab_results": []}

# Create global instance
ai_service = AIService()

