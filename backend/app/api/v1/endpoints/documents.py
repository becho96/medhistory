from fastapi import APIRouter, Depends, HTTPException, status, UploadFile, File, Query
from fastapi.responses import StreamingResponse
from sqlalchemy.ext.asyncio import AsyncSession
from typing import Optional, List
from io import BytesIO
import uuid
import logging
from urllib.parse import quote

logger = logging.getLogger(__name__)

from app.db.postgres import get_db
from app.models.user import User
from app.schemas.document import (
    Document as DocumentSchema,
    DocumentWithMetadata,
    DocumentUploadResponse
)
from app.services.document_service import DocumentService
from app.services.unit_normalization_service import unit_normalization_service
from app.services.analyte_normalization_service_db import analyte_normalization_service_db
from app.api.deps import get_current_user, get_profile_user_id
from app.db.mongodb import document_metadata_collection

router = APIRouter()

@router.post("/upload", response_model=DocumentUploadResponse)
async def upload_document(
    file: UploadFile = File(...),
    current_user: User = Depends(get_current_user),
    profile_user_id: uuid.UUID = Depends(get_profile_user_id),
    db: AsyncSession = Depends(get_db)
):
    """Upload a new document
    
    Use X-Profile-Id header to upload to a family member's profile.
    """
    
    try:
        document = await DocumentService.upload_document(
            file=file,
            user_id=profile_user_id,
            db=db
        )
        
        return DocumentUploadResponse(
            document_id=document.id,
            status=document.processing_status,
            message="–î–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è"
        )
    
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {str(e)}"
        )

@router.get("/", response_model=List[DocumentWithMetadata])
async def get_documents(
    skip: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=10000),  # Increased for timeline support
    document_type: Optional[List[str]] = Query(None),
    patient_name: Optional[List[str]] = Query(None),
    medical_facility: Optional[List[str]] = Query(None),
    date_from: Optional[str] = None,
    date_to: Optional[str] = None,
    created_from: Optional[str] = None,
    created_to: Optional[str] = None,
    sort_by: str = Query("document_date", regex="^(document_date|created_at)$"),
    # MongoDB filters
    specialties: Optional[List[str]] = Query(None),
    document_subtype: Optional[List[str]] = Query(None),
    research_area: Optional[List[str]] = Query(None),
    current_user: User = Depends(get_current_user),
    profile_user_id: uuid.UUID = Depends(get_profile_user_id),
    db: AsyncSession = Depends(get_db)
):
    """Get user documents with optional filters
    
    Supports filtering by both PostgreSQL and MongoDB fields:
    
    PostgreSQL filters:
    - document_type: filter by document types (multiple values supported)
    - patient_name: filter by patient names (multiple values supported)
    - medical_facility: filter by medical facilities (multiple values supported)
    - date_from/date_to: filter by document_date (date on the medical document)
    - created_from/created_to: filter by created_at (upload date to system)
    - sort_by: "document_date" (default) or "created_at"
    
    MongoDB filters:
    - specialties: filter by specialties (for "–ü—Ä–∏–µ–º –≤—Ä–∞—á–∞" documents)
    - document_subtype: filter by document subtype
    - research_area: filter by research area (for "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ")
    """
    
    documents = await DocumentService.get_documents(
        user_id=profile_user_id,
        db=db,
        skip=skip,
        limit=limit,
        document_type=document_type,
        patient_name=patient_name,
        medical_facility=medical_facility,
        date_from=date_from,
        date_to=date_to,
        created_from=created_from,
        created_to=created_to,
        sort_by=sort_by,
        specialties=specialties,
        document_subtype=document_subtype,
        research_area=research_area
    )
    
    # Enrich documents with MongoDB metadata
    if documents:
        doc_ids = [str(doc.id) for doc in documents if doc.mongodb_metadata_id]
        metadata_by_doc_id = {}
        
        if doc_ids:
            cursor = document_metadata_collection.find({
                "document_id": {"$in": doc_ids}
            }, {
                "document_id": 1,
                "classification.specialties": 1,
                "classification.document_subtype": 1,
                "classification.research_area": 1,
                "extracted_data.summary": 1
            })
            mongo_docs = await cursor.to_list(length=len(doc_ids))
            
            for m in mongo_docs:
                doc_id = m.get("document_id")
                metadata_by_doc_id[doc_id] = {
                    "specialties": m.get("classification", {}).get("specialties"),
                    "document_subtype": m.get("classification", {}).get("document_subtype"),
                    "research_area": m.get("classification", {}).get("research_area"),
                    "summary": m.get("extracted_data", {}).get("summary")
                }
        
        # Build response with metadata
        result = []
        for doc in documents:
            doc_dict = {
                **doc.__dict__,
            }
            
            # Add MongoDB metadata if available
            metadata = metadata_by_doc_id.get(str(doc.id), {})
            specialties = metadata.get("specialties")
            doc_dict["specialty"] = ", ".join(specialties) if specialties else None
            doc_dict["document_subtype"] = metadata.get("document_subtype")
            doc_dict["research_area"] = metadata.get("research_area")
            doc_dict["summary"] = metadata.get("summary")
            
            result.append(DocumentWithMetadata(**doc_dict))
        
        return result
    
    return []

@router.get("/count/total")
async def get_documents_count(
    document_type: Optional[List[str]] = Query(None),
    patient_name: Optional[List[str]] = Query(None),
    medical_facility: Optional[List[str]] = Query(None),
    date_from: Optional[str] = None,
    date_to: Optional[str] = None,
    created_from: Optional[str] = None,
    created_to: Optional[str] = None,
    # MongoDB filters
    specialties: Optional[List[str]] = Query(None),
    document_subtype: Optional[List[str]] = Query(None),
    research_area: Optional[List[str]] = Query(None),
    current_user: User = Depends(get_current_user),
    profile_user_id: uuid.UUID = Depends(get_profile_user_id),
    db: AsyncSession = Depends(get_db)
):
    """Get total count of user documents with optional filters"""
    
    count = await DocumentService.get_documents_count(
        user_id=profile_user_id,
        db=db,
        document_type=document_type,
        patient_name=patient_name,
        medical_facility=medical_facility,
        date_from=date_from,
        date_to=date_to,
        created_from=created_from,
        created_to=created_to,
        specialties=specialties,
        document_subtype=document_subtype,
        research_area=research_area
    )
    
    return {"total": count}

@router.get("/{document_id}", response_model=DocumentWithMetadata)
async def get_document(
    document_id: uuid.UUID,
    current_user: User = Depends(get_current_user),
    profile_user_id: uuid.UUID = Depends(get_profile_user_id),
    db: AsyncSession = Depends(get_db)
):
    """Get document by ID with MongoDB metadata"""
    
    document = await DocumentService.get_document_by_id(
        document_id=document_id,
        user_id=profile_user_id,
        db=db
    )
    
    if not document:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"
        )
    
    # Enrich with MongoDB metadata
    doc_dict = {
        "id": document.id,
        "user_id": document.user_id,
        "original_filename": document.original_filename,
        "file_size": document.file_size,
        "file_type": document.file_type,
        "file_url": document.file_url,
        "document_type": document.document_type,
        "document_date": document.document_date,
        "patient_name": document.patient_name,
        "medical_facility": document.medical_facility,
        "processing_status": document.processing_status,
        "mongodb_metadata_id": document.mongodb_metadata_id,
        "created_at": document.created_at,
        "updated_at": document.updated_at,
        "specialty": None,
        "document_subtype": None,
        "research_area": None,
        "summary": None
    }
    
    # Load MongoDB metadata if available
    if document.mongodb_metadata_id:
        mongo_doc = await document_metadata_collection.find_one({
            "document_id": str(document.id)
        })
        
        if mongo_doc:
            classification = mongo_doc.get("classification", {})
            extracted_data = mongo_doc.get("extracted_data", {})
            
            specialties = classification.get("specialties")
            doc_dict["specialty"] = ", ".join(specialties) if specialties else None
            doc_dict["document_subtype"] = classification.get("document_subtype")
            doc_dict["research_area"] = classification.get("research_area")
            doc_dict["summary"] = extracted_data.get("summary")
    
    return DocumentWithMetadata(**doc_dict)

@router.get("/{document_id}/file")
async def download_document(
    document_id: uuid.UUID,
    current_user: User = Depends(get_current_user),
    profile_user_id: uuid.UUID = Depends(get_profile_user_id),
    db: AsyncSession = Depends(get_db)
):
    """Download document file"""
    
    document = await DocumentService.get_document_by_id(
        document_id=document_id,
        user_id=profile_user_id,
        db=db
    )
    
    if not document:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"
        )
    
    # Get file from MinIO
    try:
        print(f"üì• Downloading file: {document.file_url}")
        print(f"   Document ID: {document.id}")
        print(f"   Filename: {document.original_filename}")
        
        file_content = DocumentService.get_file_from_minio(document.file_url)
        
        print(f"   ‚úÖ File downloaded: {len(file_content)} bytes")
        
        # Determine content type
        content_types = {
            'pdf': 'application/pdf',
            'jpg': 'image/jpeg',
            'jpeg': 'image/jpeg',
            'png': 'image/png',
            'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
        }
        
        content_type = content_types.get(document.file_type, 'application/octet-stream')
        
        print(f"   Content-Type: {content_type}")
        print(f"   Creating StreamingResponse...")
        
        # Encode filename for Content-Disposition header (RFC 5987)
        # This supports UTF-8 filenames including Cyrillic
        encoded_filename = quote(document.original_filename)
        
        return StreamingResponse(
            BytesIO(file_content),
            media_type=content_type,
            headers={
                "Content-Disposition": f"attachment; filename*=UTF-8''{encoded_filename}"
            }
        )
    
    except Exception as e:
        print(f"‚ùå Error downloading file:")
        print(f"   Document ID: {document_id}")
        print(f"   File URL: {document.file_url if document else 'N/A'}")
        print(f"   Error: {str(e)}")
        import traceback
        traceback.print_exc()
        
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞: {str(e)}"
        )

@router.delete("/{document_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_document(
    document_id: uuid.UUID,
    current_user: User = Depends(get_current_user),
    profile_user_id: uuid.UUID = Depends(get_profile_user_id),
    db: AsyncSession = Depends(get_db)
):
    """Delete document"""
    
    success = await DocumentService.delete_document(
        document_id=document_id,
        user_id=profile_user_id,
        db=db
    )
    
    if not success:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"
        )
    
    return None


@router.get("/{document_id}/labs")
async def get_document_labs(
    document_id: uuid.UUID,
    current_user: User = Depends(get_current_user),
    profile_user_id: uuid.UUID = Depends(get_profile_user_id),
    db: AsyncSession = Depends(get_db),
):
    """Return extracted lab results for a document from MongoDB."""

    document = await DocumentService.get_document_by_id(
        document_id=document_id,
        user_id=profile_user_id,
        db=db,
    )

    if not document:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω",
        )

    mongo_doc = await document_metadata_collection.find_one({
        "document_id": str(document_id)
    })

    lab_results = (mongo_doc or {}).get("extracted_data", {}).get("lab_results", [])

    return {
        "document_id": str(document_id),
        "lab_results": lab_results,
    }


@router.get("/{document_id}/labs/summary")
async def get_document_labs_summary(
    document_id: uuid.UUID,
    current_user: User = Depends(get_current_user),
    profile_user_id: uuid.UUID = Depends(get_profile_user_id),
    db: AsyncSession = Depends(get_db),
):
    """Return quick summary whether labs exist and how many."""

    document = await DocumentService.get_document_by_id(
        document_id=document_id,
        user_id=profile_user_id,
        db=db,
    )

    if not document:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω",
        )

    mongo_doc = await document_metadata_collection.find_one({
        "document_id": str(document_id)
    }, {"extracted_data.lab_results": 1})

    lab_results = (mongo_doc or {}).get("extracted_data", {}).get("lab_results", [])
    count = len(lab_results or [])

    return {
        "document_id": str(document_id),
        "has_labs": count > 0,
        "count": count,
    }


@router.get("/labs/analytes")
async def list_available_analytes(
    current_user: User = Depends(get_current_user),
    profile_user_id: uuid.UUID = Depends(get_profile_user_id),
    db: AsyncSession = Depends(get_db),
):
    """Return distinct analyte names grouped by categories with standard units.
    
    Response format:
    {
        "categories": [
            {
                "name": "–û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –∫—Ä–æ–≤–∏",
                "analytes": [
                    {"canonical_name": "–ì–µ–º–æ–≥–ª–æ–±–∏–Ω", "standard_unit": "–≥/–ª", "count": 5},
                    ...
                ]
            },
            ...
        ]
    }
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∂–µ–Ω –ª–∏ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫. –ï—Å–ª–∏ –Ω–µ—Ç - –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å
    if not analyte_normalization_service_db.is_loaded:
        logger.warning("‚ö†Ô∏è –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –∞–Ω–∞–ª–∏–∑–æ–≤ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω, –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å...")
        try:
            await analyte_normalization_service_db.load_from_db(db)
            logger.info(f"‚úÖ –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –∑–∞–≥—Ä—É–∂–µ–Ω: {analyte_normalization_service_db.get_stats()}")
        except Exception as e:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫: {e}")
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –Ω–∞–∑–≤–∞–Ω–∏–π –∞–Ω–∞–ª–∏–∑–æ–≤ –∏ –µ–¥–∏–Ω–∏—Ü –∏–∑ MongoDB
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ (test_name, unit) —á—Ç–æ–±—ã —Ä–∞–∑–ª–∏—á–∞—Ç—å –Ω–∞–ø—Ä–∏–º–µ—Ä "–õ–∏–º—Ñ–æ—Ü–∏—Ç—ã %" –∏ "–õ–∏–º—Ñ–æ—Ü–∏—Ç—ã –∞–±—Å"
    pipeline = [
        {"$match": {"user_id": str(profile_user_id)}},
        {"$project": {"extracted_data.lab_results": 1}},
        {"$unwind": "$extracted_data.lab_results"},
        {
            "$group": {
                "_id": {
                    "name_lower": {"$toLower": "$extracted_data.lab_results.test_name"},
                    "unit": "$extracted_data.lab_results.unit"
                },
                "name": {"$first": "$extracted_data.lab_results.test_name"},
                "unit": {"$first": "$extracted_data.lab_results.unit"},
                "count": {"$sum": 1}
            }
        },
        {"$sort": {"name": 1}},
    ]
    
    cursor = document_metadata_collection.aggregate(pipeline)
    
    # –°–æ–±–∏—Ä–∞–µ–º –∞–Ω–∞–ª–∏–∑—ã –∏ –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–º –Ω–∞–∑–≤–∞–Ω–∏—è–º
    canonical_analytes = {}  # canonical_name -> {count, standard_unit, category}
    unknown_analytes = []  # –ê–Ω–∞–ª–∏–∑—ã –±–µ–∑ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è
    
    async for doc in cursor:
        original_name = doc.get("name", "")
        unit = doc.get("unit", "")
        count = doc.get("count", 0)
        
        if not original_name:
            continue
        
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å —É—á—ë—Ç–æ–º –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è
        canonical_name = analyte_normalization_service_db.get_canonical_name(original_name, unit)
        
        if canonical_name:
            analyte_data = analyte_normalization_service_db.get_analyte(canonical_name)
            
            if canonical_name in canonical_analytes:
                # –°—É–º–º–∏—Ä—É–µ–º count –¥–ª—è —Å–∏–Ω–æ–Ω–∏–º–æ–≤
                canonical_analytes[canonical_name]["count"] += count
            else:
                canonical_analytes[canonical_name] = {
                    "canonical_name": canonical_name,
                    "standard_unit": analyte_data.standard_unit if analyte_data else None,
                    "category": analyte_data.category_name if analyte_data else "–î—Ä—É–≥–∏–µ –∞–Ω–∞–ª–∏–∑—ã",
                    "count": count
                }
        else:
            # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ - –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É –Ω–∞–∑–≤–∞–Ω–∏—é
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏—é name+unit –∫–∞–∫ –∫–ª—é—á –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            unknown_key = f"{original_name}|{unit or ''}"
            existing = next((a for a in unknown_analytes if f"{a['canonical_name']}|{a.get('_unit', '')}" == unknown_key), None)
            if existing:
                existing["count"] += count
            else:
                unknown_analytes.append({
                    "canonical_name": original_name,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
                    "standard_unit": unit,
                    "category": "–î—Ä—É–≥–∏–µ –∞–Ω–∞–ª–∏–∑—ã",
                    "count": count,
                    "_unit": unit  # –í—Ä–µ–º–µ–Ω–Ω–æ–µ –ø–æ–ª–µ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
                })
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –∞–Ω–∞–ª–∏–∑—ã
    all_analytes = list(canonical_analytes.values()) + unknown_analytes
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    categories_dict = {}
    for analyte in all_analytes:
        category = analyte.get("category", "–î—Ä—É–≥–æ–µ")
        if category not in categories_dict:
            categories_dict[category] = []
        categories_dict[category].append({
            "canonical_name": analyte["canonical_name"],
            "standard_unit": analyte["standard_unit"],
            "count": analyte["count"]
        })
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É –∏–∑ –ë–î
    db_categories = analyte_normalization_service_db.get_all_categories()
    category_order = [c["name"] for c in db_categories]
    result_categories = []
    
    for category_name in category_order:
        if category_name in categories_dict:
            result_categories.append({
                "name": category_name,
                "analytes": sorted(
                    categories_dict[category_name],
                    key=lambda x: x["canonical_name"]
                )
            })
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –ø–æ—Ä—è–¥–∫–µ (–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ)
    for category_name, analytes_list in categories_dict.items():
        if category_name not in category_order:
            result_categories.append({
                "name": category_name,
                "analytes": sorted(analytes_list, key=lambda x: x["canonical_name"])
            })
    
    return {"categories": result_categories}


@router.get("/labs/analytes/debug")
async def debug_analytes_mapping(
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """Debug endpoint to check analyte normalization service status."""
    stats = analyte_normalization_service_db.get_stats()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã –º–∞–ø–ø–∏–Ω–≥–∞
    test_names = ["–õ–∏–º—Ñ–æ—Ü–∏—Ç—ã", "–ì–µ–º–æ–≥–ª–æ–±–∏–Ω", "Hemoglobin", "HGB", "–ì–ª—é–∫–æ–∑–∞"]
    test_results = {}
    for name in test_names:
        canonical = analyte_normalization_service_db.get_canonical_name(name)
        test_results[name] = canonical
    
    return {
        "service_stats": stats,
        "test_mappings": test_results,
        "all_categories": analyte_normalization_service_db.get_all_categories(),
    }


@router.get("/labs/timeseries")
async def get_lab_timeseries(
    analyte: str = Query(..., description="–ö–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä: –ì–µ–º–æ–≥–ª–æ–±–∏–Ω"),
    current_user: User = Depends(get_current_user),
    profile_user_id: uuid.UUID = Depends(get_profile_user_id),
    db: AsyncSession = Depends(get_db),
):
    """Return time series for a given analyte across user's documents.
    
    All values are converted to standard units automatically.
    
    Response:
    {
        "analyte": "–ì–µ–º–æ–≥–ª–æ–±–∏–Ω",
        "standard_unit": "–≥/–ª",
        "category": "–û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –∫—Ä–æ–≤–∏",
        "points": [
            {
                "date": "2024-01-15",
                "value_num": 145.0,
                "unit": "–≥/–ª",
                "document_id": "...",
                "reference_range": "120-160",
                "flag": "N"
            }
        ]
    }
    """
    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞
    analyte_data = analyte_normalization_service_db.get_analyte(analyte)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —ç—Ç–æ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–π –∏–ª–∏ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –∞–Ω–∞–ª–∏–∑–∞
    # –ù–∞–∑–≤–∞–Ω–∏—è –≤ –ë–î —Å –ø—Ä–æ–±–µ–ª–æ–º: "–õ–∏–º—Ñ–æ—Ü–∏—Ç—ã (%)", "–õ–∏–º—Ñ–æ—Ü–∏—Ç—ã (–∞–±—Å)"
    is_percentage_analyte = analyte.endswith(" (%)")
    is_absolute_analyte = analyte.endswith(" (–∞–±—Å)")
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
    if analyte_data:
        synonyms = list(analyte_data.synonyms) if analyte_data.synonyms else [analyte]
        standard_unit = analyte_data.standard_unit
        category = analyte_data.category_name
        
        # –î–ª—è –∞–Ω–∞–ª–∏–∑–æ–≤ —Å –¥–≤–æ–π–Ω—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ (% –∏ –∞–±—Å) –¥–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã –ø–∞—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        # –≠—Ç–æ –Ω—É–∂–Ω–æ –ø–æ—Ç–æ–º—É —á—Ç–æ –≤ MongoDB –º–æ–∂–µ—Ç –±—ã—Ç—å "–ù–µ–π—Ç—Ä–æ—Ñ–∏–ª—ã" —Å unit="%" 
        # –∞ –Ω–µ "–ù–µ–π—Ç—Ä–æ—Ñ–∏–ª—ã %" –∫–∞–∫ —Å–∏–Ω–æ–Ω–∏–º
        dual_pairs = {
            "–õ–∏–º—Ñ–æ—Ü–∏—Ç—ã (%)": "–õ–∏–º—Ñ–æ—Ü–∏—Ç—ã (–∞–±—Å)",
            "–õ–∏–º—Ñ–æ—Ü–∏—Ç—ã (–∞–±—Å)": "–õ–∏–º—Ñ–æ—Ü–∏—Ç—ã (%)",
            "–ù–µ–π—Ç—Ä–æ—Ñ–∏–ª—ã (%)": "–ù–µ–π—Ç—Ä–æ—Ñ–∏–ª—ã (–∞–±—Å)",
            "–ù–µ–π—Ç—Ä–æ—Ñ–∏–ª—ã (–∞–±—Å)": "–ù–µ–π—Ç—Ä–æ—Ñ–∏–ª—ã (%)",
            "–ú–æ–Ω–æ—Ü–∏—Ç—ã (%)": "–ú–æ–Ω–æ—Ü–∏—Ç—ã (–∞–±—Å)",
            "–ú–æ–Ω–æ—Ü–∏—Ç—ã (–∞–±—Å)": "–ú–æ–Ω–æ—Ü–∏—Ç—ã (%)",
            "–≠–æ–∑–∏–Ω–æ—Ñ–∏–ª—ã (%)": "–≠–æ–∑–∏–Ω–æ—Ñ–∏–ª—ã (–∞–±—Å)",
            "–≠–æ–∑–∏–Ω–æ—Ñ–∏–ª—ã (–∞–±—Å)": "–≠–æ–∑–∏–Ω–æ—Ñ–∏–ª—ã (%)",
            "–ë–∞–∑–æ—Ñ–∏–ª—ã (%)": "–ë–∞–∑–æ—Ñ–∏–ª—ã (–∞–±—Å)",
            "–ë–∞–∑–æ—Ñ–∏–ª—ã (–∞–±—Å)": "–ë–∞–∑–æ—Ñ–∏–ª—ã (%)",
        }
        
        if analyte in dual_pairs:
            paired_analyte = dual_pairs[analyte]
            paired_data = analyte_normalization_service_db.get_analyte(paired_analyte)
            if paired_data and paired_data.synonyms:
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã –ø–∞—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
                synonyms.extend(paired_data.synonyms)
                # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                synonyms = list(set(synonyms))
    else:
        # –ï—Å–ª–∏ –∞–Ω–∞–ª–∏–∑ –Ω–µ –≤ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–µ, –∏—â–µ–º –ø–æ —Ç–æ—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é
        synonyms = [analyte]
        standard_unit = None
        category = "–î—Ä—É–≥–∏–µ –∞–Ω–∞–ª–∏–∑—ã"
    
    # –°—Ç—Ä–æ–∏–º regex –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—Å–µ—Ö —Å–∏–Ω–æ–Ω–∏–º–æ–≤
    # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –≤ –Ω–∞–∑–≤–∞–Ω–∏—è—Ö
    import re
    escaped_synonyms = [re.escape(s) for s in synonyms]
    regex_pattern = f"^({'|'.join(escaped_synonyms)})$"
    
    pipeline = [
        {"$match": {"user_id": str(profile_user_id)}},
        {"$project": {"document_id": 1, "extracted_data.lab_results": 1}},
        {"$unwind": "$extracted_data.lab_results"},
        {"$match": {"extracted_data.lab_results.test_name": {"$regex": regex_pattern, "$options": "i"}}},
    ]

    cursor = document_metadata_collection.aggregate(pipeline)
    points = []
    doc_ids = set()
    
    async for doc in cursor:
        lr = doc.get("extracted_data", {}).get("lab_results", {})
        if not isinstance(lr, dict):
            continue
        
        doc_id = doc.get("document_id")
        original_value = lr.get("value")
        original_unit = lr.get("unit") or ""
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–∏–ø—É –∞–Ω–∞–ª–∏–∑–∞ (–ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–π/–∞–±—Å–æ–ª—é—Ç–Ω—ã–π)
        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ - –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–∏ —Å unit —Å–æ–¥–µ—Ä–∂–∞—â–∏–º %
        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–ø–∏—Å–∏ —Å unit —Å–æ–¥–µ—Ä–∂–∞—â–∏–º %
        unit_has_percent = "%" in original_unit
        
        if is_percentage_analyte and not unit_has_percent:
            # –ó–∞–ø—Ä–æ—à–µ–Ω –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–π, –Ω–æ unit –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç % - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            continue
        if is_absolute_analyte and unit_has_percent:
            # –ó–∞–ø—Ä–æ—à–µ–Ω –∞–±—Å–æ–ª—é—Ç–Ω—ã–π, –Ω–æ unit —Å–æ–¥–µ—Ä–∂–∏—Ç % - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            continue
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –µ–¥–∏–Ω–∏—Ü—É
        if analyte_data:
            converted_value, converted_unit = analyte_normalization_service_db.convert_value(
                original_value, original_unit, analyte
            )
        else:
            # –î–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–æ–≤ - –ø—Ä–æ—Å—Ç–æ –ø–∞—Ä—Å–∏–º —á–∏—Å–ª–æ
            try:
                converted_value = float(str(original_value).replace(',', '.').strip())
            except (ValueError, TypeError):
                converted_value = None
            converted_unit = original_unit
        
        if converted_value is None:
            continue
        
        if doc_id:
            doc_ids.add(doc_id)
            
        points.append({
            "document_id": doc_id,
            "value_num": converted_value,
            "unit": converted_unit or standard_unit,
            "original_value": original_value,
            "original_unit": original_unit,
            "reference_range": lr.get("reference_range"),
            "flag": lr.get("flag"),
        })

    # Fetch dates for documents from Postgres
    if doc_ids:
        import uuid as _uuid
        from sqlalchemy import select
        from app.models.document import Document as DocumentModel
        q = select(DocumentModel.id, DocumentModel.document_date).where(
            DocumentModel.id.in_([_uuid.UUID(x) for x in doc_ids])
        )
        result = await db.execute(q)
        id_to_date = {str(r[0]): r[1] for r in result.all()}
        for p in points:
            p["date"] = id_to_date.get(p["document_id"])

    # Sort by date
    points.sort(key=lambda x: (x.get("date") is None, x.get("date") or ""))

    return {
        "analyte": analyte,
        "standard_unit": standard_unit,
        "category": category,
        "points": points
    }


@router.get("/filters/values")
async def get_filter_values(
    field: str = Query(..., description="Field name: document_type, patient_name, medical_facility, specialties, document_subtype, research_area"),
    q: Optional[str] = Query(None, description="Search query to filter values"),
    limit: int = Query(50, ge=1, le=100),
    current_user: User = Depends(get_current_user),
    profile_user_id: uuid.UUID = Depends(get_profile_user_id),
    db: AsyncSession = Depends(get_db),
):
    """Get distinct values for filter fields.
    
    Supports both PostgreSQL fields (document_type, patient_name, medical_facility)
    and MongoDB fields (specialties, document_subtype, research_area).
    
    Query parameter 'q' can be used to search/filter the values.
    """
    
    # PostgreSQL fields
    postgres_fields = {"document_type", "patient_name", "medical_facility"}
    # MongoDB fields
    mongodb_fields = {"specialties", "document_subtype", "research_area"}
    
    if field in postgres_fields:
        values = await DocumentService.get_distinct_field_values(
            user_id=profile_user_id,
            db=db,
            field=field,
            q=q,
            limit=limit
        )
        return {"field": field, "values": values}
    
    elif field in mongodb_fields:
        values = await DocumentService.get_distinct_mongodb_field_values(
            user_id=profile_user_id,
            field=field,
            q=q,
            limit=limit
        )
        return {"field": field, "values": values}
    
    else:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Invalid field: {field}. Allowed fields: {', '.join(postgres_fields | mongodb_fields)}"
        )

